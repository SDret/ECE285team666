{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import all the package needed\n",
    "#original\n",
    "#use augmention\n",
    "#use dropout\n",
    "#remove the dropout layer before the final layer\n",
    "#add more FC layers for hair color recognition\n",
    "# torch.nn.ConvTranspose2d can do unsampling and can be regarded as a deconvolution operation.\n",
    "from sklearn.manifold import TSNE\n",
    "from __future__ import print_function\n",
    "import torch\n",
    "import numpy as np\n",
    "import MNISTtools as mit\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F \n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.autograd import Variable\n",
    "from __future__ import print_function\n",
    "import torch.utils.data as Data\n",
    "import torchvision\n",
    "import os\n",
    "import matplotlib.pylab as plt\n",
    "# import Augmentor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net_1(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net_1,self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3,6,3)\n",
    "        torch.nn.init.xavier_uniform(self.conv1.weight)\n",
    "        self.conv1_bn = nn.BatchNorm2d(6)\n",
    "        \n",
    "        self.conv2 = nn.Conv2d (6,16,3)\n",
    "        torch.nn.init.xavier_uniform(self.conv2.weight)\n",
    "        self.conv2_bn = nn.BatchNorm2d(16)\n",
    "        \n",
    "        self.conv3 = nn.Conv2d(16,30,3)\n",
    "        torch.nn.init.xavier_uniform(self.conv2.weight)\n",
    "        self.conv3_bn = nn.BatchNorm2d(30)   \n",
    "        \n",
    "        self.conv4 = nn.Conv2d(30,80,3)\n",
    "        torch.nn.init.xavier_uniform(self.conv2.weight)\n",
    "        self.conv4_bn = nn.BatchNorm2d(80)\n",
    "        \n",
    "        self.conv5 = nn.Conv2d(80,100,3)\n",
    "        torch.nn.init.xavier_uniform(self.conv2.weight)\n",
    "        self.conv5_bn = nn.BatchNorm2d(100)\n",
    "###fully connected layer \n",
    "        \n",
    "        self.fc1 = nn.Linear(1200,1000)\n",
    "        self.fc1_drop = nn.Dropout()\n",
    "        torch.nn.init.xavier_uniform(self.fc1.weight)\n",
    "        self.fc1_bn = nn.BatchNorm2d(1000)  \n",
    "        self.fc2 = nn.Linear (1000,800)\n",
    "        self.fc2_drop = nn.Dropout()\n",
    "        torch.nn.init.xavier_uniform(self.fc2.weight)\n",
    "        self.fc2_bn = nn.BatchNorm2d(800)    \n",
    "        self.fc3 = nn.Linear (800,500)\n",
    "        torch.nn.init.xavier_uniform(self.fc3.weight)\n",
    "        self.fc3_bn = nn.BatchNorm2d(500)       \n",
    "        self.fc4 = nn.Linear (500,4)\n",
    "        torch.nn.init.xavier_uniform(self.fc3.weight)\n",
    "        \n",
    "###forward \n",
    "    def forward(self,x):\n",
    "        x = F.max_pool2d(F.relu(self.conv1_bn(self.conv1(x))),(2,2))\n",
    "        x = F.max_pool2d(F.relu(self.conv2_bn(self.conv2(x))),(2,2))\n",
    "        x = F.max_pool2d(F.relu(self.conv3_bn(self.conv3(x))),(2,2))\n",
    "        x = F.max_pool2d(F.relu(self.conv4_bn(self.conv4(x))),(2,2))\n",
    "        x = F.max_pool2d(F.relu(self.conv5_bn(self.conv5(x))),(2,2))\n",
    "        x = x.view(-1,self.num_flat_features(x))\n",
    "        #print (x.size())\n",
    "        x = self.fc1_drop(F.relu(self.fc1_bn(self.fc1(x))))\n",
    "        x = self.fc2_drop(F.relu(self.fc2_bn(self.fc2(x))))\n",
    "        x = F.relu(self.fc3_bn(self.fc3(x)))\n",
    "        output = self.fc4(x)\n",
    "        #output = x\n",
    "        return output\n",
    "    \n",
    "    def num_flat_features(self,x):\n",
    "        size = x.size()[1:]\n",
    "        return np.prod(size)\n",
    "    \n",
    "def label2onehot(lbl):                     \n",
    "    d = np.zeros((lbl.max() + 1, lbl.size)) \n",
    "    d[lbl, np.arange(0, lbl.size)] = 1 \n",
    "    return d\n",
    "\n",
    "#this function is written for assignment#2,so to be modified\n",
    "def normalize_MNIST_images(x):\n",
    "    num_f = x.shape[0]\n",
    "    x=x.astype(np.float64)\n",
    "    mean_each_feature = np.mean(x,axis = 1).reshape(num_f,1)\n",
    "    result = (x - mean_each_feature)/255\n",
    "    return result,mean_each_feature\n",
    "\n",
    "def accuracy(prediction,lable):\n",
    "    assert len(prediction) == len(lable)\n",
    "    return np.sum(prediction == lable).astype(float)/len(prediction)\n",
    "    \n",
    "def onehot2label(d):\n",
    "    lbl = d.argmax(axis=0) \n",
    "    return lbl    \n",
    "\n",
    "def Get_Desktop_Path():\n",
    "    return os.path.join(os.path.expanduser(\"~\"), 'Desktop')\n",
    "\n",
    "def load_images(index_to_use):\n",
    "    im = np.zeros((218,178,3,len(index_to_use)))\n",
    "    for i in range(202599):\n",
    "        if i in index_to_use:\n",
    "            index = i\n",
    "            i = '0'*(6-len(str(i+1)))+str(i+1)\n",
    "            im[:,:,:,index_to_use.index(index)] = plt.imread('/datasets/ee285s-public/CelebA/Img/img_align_celeba/%s.jpg'%i)\n",
    "    return im\n",
    "\n",
    "#do the selection and preprocessing for labels,also could get which images(images' index) within dataset is qualified\n",
    "def load_labels():\n",
    "    # Bald             4\n",
    "    # Black_Hair       8 \n",
    "    # Blond_Hair       9      \n",
    "    # Brown_Hair       11\n",
    "    # Gray_Hair       17\n",
    "    # Straight_Hair   32\n",
    "    # Wavy_Hair       33\n",
    "    # Wearing_Hat     35\n",
    "    with open('/datasets/ee285s-public/CelebA/Anno/list_attr_celeba.txt','r')  as f:\n",
    "        p_temp=f.readlines()\n",
    "    p = []\n",
    "    for i in p_temp:\n",
    "        i = i[11:]\n",
    "        i = i[:-2]\n",
    "        i = i.replace(' ','')\n",
    "        i = i.replace('-1','0')\n",
    "        p.append(i)\n",
    "    p = p[2:]   \n",
    "    ylabel_temp = np.zeros((202599,40))\n",
    "    index = 0\n",
    "    for each in p:\n",
    "        temp_x = []\n",
    "        for i in each:\n",
    "            temp_x.append(int(i))\n",
    "        ylabel_temp[index,:] = np.array(temp_x)\n",
    "        index += 1\n",
    "    ylabel_for_color = ylabel_temp[:,[8,9,11,17]]\n",
    "    ylabel_for_style = ylabel_temp[:,[4,32,33,35]]\n",
    "    index_to_delete = image_to_abandon(ylabel_for_color,ylabel_for_style)\n",
    "    index_to_use = np.delete(np.arange(202599),index_to_delete)\n",
    "    return ylabel_for_color,ylabel_for_style,index_to_use.tolist()\n",
    "\n",
    "def image_to_abandon(ylabel_for_color,ylabel_for_style):\n",
    "    which_images_to_delete_1_1 = np.where(np.sum(ylabel_for_color,1) == 0)[0].tolist()\n",
    "    which_images_to_delete_1_2 = np.where(np.sum(ylabel_for_style,1) == 0)[0].tolist()\n",
    "    which_images_to_delete_2_1 = np.where(np.sum(ylabel_for_color,1) == 2)[0].tolist()\n",
    "    which_images_to_delete_2_2 = np.where(np.sum(ylabel_for_color,1) == 3)[0].tolist()\n",
    "    which_images_to_delete_2_3 = np.where(np.sum(ylabel_for_color,1) == 4)[0].tolist()\n",
    "    which_images_to_delete_3_1 = np.where(np.sum(ylabel_for_style,1) == 2)[0].tolist()\n",
    "    which_images_to_delete_3_2 = np.where(np.sum(ylabel_for_style,1) == 3)[0].tolist()\n",
    "    which_images_to_delete_3_3 = np.where(np.sum(ylabel_for_style,1) == 4)[0].tolist()\n",
    "    result = set(which_images_to_delete_1_1 + which_images_to_delete_1_2 + which_images_to_delete_2_1 + which_images_to_delete_2_2 + which_images_to_delete_2_3 + which_images_to_delete_3_1 + which_images_to_delete_3_2 + which_images_to_delete_3_3)\n",
    "    return list(result)\n",
    "\n",
    "#to randomly assign images' index into different batches per epoch\n",
    "def get_the_step_indexassignment(train_family,batch):\n",
    "    index_to_use = train_family\n",
    "    np.random.shuffle(index_to_use)\n",
    "    per_batch = family_of_index_to_use(index_to_use,batch)\n",
    "    num_batch = len(per_batch)\n",
    "    return per_batch,num_batch\n",
    "\n",
    "#a subfunction within get_the_step_indexassignment,it's function is grouping the selected index\n",
    "#into different batchs.\n",
    "def family_of_index_to_use(index_to_use,batch = 1000):\n",
    "    start = batch\n",
    "    end = start + batch\n",
    "    family_of_index_to_use = [index_to_use[0:batch]]\n",
    "    while len(family_of_index_to_use[-1]) >= batch:\n",
    "        family_of_index_to_use.append(index_to_use[start:end])\n",
    "        start += batch\n",
    "        end += batch\n",
    "    family_of_index_to_use = family_of_index_to_use[:-1]\n",
    "    return family_of_index_to_use\n",
    "    \n",
    "#use the assigned index to call the selected images from dataset.  \n",
    "def get_x_for_thisbatch(perbatch_index,signal):\n",
    "    if signal == False:\n",
    "        x = load_images(perbatch_index)\n",
    "        x = torch.from_numpy(np.moveaxis(np.moveaxis(x,-1,0),-1,1)).float()\n",
    "    else:\n",
    "        x = load_created_images(perbatch_index)\n",
    "        x = torch.from_numpy(np.moveaxis(np.moveaxis(x,-1,0),-1,1)).float()\n",
    "    return x\n",
    "\n",
    "def load_created_images(perbatch_index):\n",
    "    im = np.zeros((218,178,3,len(index_to_use)))\n",
    "    for i in range(202599):\n",
    "        if i in index_to_use:\n",
    "            index = i\n",
    "            i = '0'*(6-len(str(i+1)))+str(i+1)\n",
    "            im[:,:,:,index_to_use.index(index)] = plt.imread('/datasets/ee285s-public/CelebA/Augmentation/%s.jpg'%i)\n",
    "    return im\n",
    "\n",
    "#call the labels    \n",
    "def get_y_for_thisbatch(perbatch_index,ylabel):\n",
    "    y = ylabel[perbatch_index,:].argmax(axis=1)\n",
    "    y = torch.from_numpy(y).type(torch.LongTensor)  \n",
    "    return y\n",
    "\n",
    "def average(x):\n",
    "    result = []\n",
    "    for each in x:\n",
    "        temp = each/2\n",
    "        result.append(temp)\n",
    "    return result\n",
    "\n",
    "def do_some_statistics(prediction,label,chars):\n",
    "    result = []\n",
    "    for i in range(len(chars)):\n",
    "        result.append(0)\n",
    "    mis_pre = label[np.where((prediction != label) == 1)[0].tolist()]\n",
    "    i = 0\n",
    "    for each in chars:\n",
    "        for pre in mis_pre:\n",
    "            if pre == each:\n",
    "                result[i] += 1\n",
    "        i+=1\n",
    "    return result\n",
    "        \n",
    "#this part of code is written to do the image augmentation locally\n",
    "def augmentor():    \n",
    "    p = Augmentor.Pipeline('/datasets/ee285s-public/CelebA/Img/img_align_celeba')\n",
    "    p.random_distortion(probability=0.5, grid_width=4, grid_height=4, magnitude=8)\n",
    "    p.rotate(probability=0.7, max_left_rotation=10, max_right_rotation=10)\n",
    "    p.flip_left_right(probability=1)\n",
    "    p.zoom(probability=0.5, min_factor=1.2, max_factor=1.3)\n",
    "    p.gaussian_distortion(probability = 1,grid_width = 5,grid_height = 5,magnitude = 5,corner = 'bell',method = 'in')\n",
    "    p.skew(probability = 0.5)\n",
    "    p.process()\n",
    "    p.sample(20)\n",
    "    \n",
    "def batch_rename_for_augmentor():\n",
    "    i=0\n",
    "    path='/Users/sdret/Desktop/image/img_align_celeba/output'\n",
    "    filelist=os.listdir(path)\n",
    "    for files in filelist:\n",
    "        i=i+1\n",
    "        Olddir=os.path.join(path,files)\n",
    "        filename=os.path.splitext(files)[0]\n",
    "        filetype=os.path.splitext(files)[1]\n",
    "        Newdir=os.path.join('/Users/sdret/Desktop/image/img_align_celeba/output',files[26:32]+filetype);\n",
    "        os.rename(Olddir,Newdir)\n",
    "\n",
    "def get_num_of_each_labels(index_family,label_type):\n",
    "    if label_type == 'ylabel_for_color':\n",
    "        temp = get_y_for_thisbatch(index_family,ylabel_for_color).numpy()\n",
    "    else:\n",
    "        temp = get_y_for_thisbatch(index_family,ylabel_for_style).numpy()\n",
    "    return np.sum(label2onehot(temp),1)\n",
    "\n",
    "def get_the_mis_ratio(result,col_or_sty):\n",
    "    if col_or_sty == 'col':\n",
    "        num_test_col = np.array([4993,3776,5048,700])\n",
    "        outcome = result / num_test_col\n",
    "    else:\n",
    "        num_test_sty = np.array([211,5617,8564,125])\n",
    "        outcome = result / num_test_sty\n",
    "    return outcome\n",
    "\n",
    "def select_images_for_hat(family,ylabel,index):\n",
    "    list = []\n",
    "    for each in family:\n",
    "        if get_y_for_thisbatch([each],ylabel)[0] == index:\n",
    "            list.append(each + 1)\n",
    "    return list\n",
    "        \n",
    "#this part of code is to get the path of target file on pod\n",
    "# def processDirectory (args, dirname, filenames):\n",
    "#     print 'Directory',dirname\n",
    "#     for filename in filenames:\n",
    "#         print ' File',filename"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# os.path.walk(r'/datasets/ee285s-public/CelebA', processDirectory, None )\n",
    "test_1 = plt.imread('/datasets/ee285s-public/CelebA/Img/img_align_celeba/202598.jpg')\n",
    "test_2 = plt.imread('/datasets/ee285s-public/CelebA/Augmentation/202598.jpg')\n",
    "plt.imshow(test_1)\n",
    "plt.imshow(test_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#because the dataset we used are not initially suitable for our project,so we do some preprocessing &\n",
    "#selection for it,all the first-step processing are included in the function load_labels()\n",
    "#index_to_use is the index of image qualified for future use.\n",
    "ylabel_for_color,ylabel_for_style,index_to_use = load_labels()\n",
    "#for the randomness,we do the shuffling to the index\n",
    "np.random.shuffle(index_to_use)\n",
    "#because the limitaion of the cpu,we can not load all the images and into matrix form,instead\n",
    "#when we need them,we load the data.\n",
    "\n",
    "##separate the data into train & validation & test sets\n",
    "##after the first selection,one must lock the this code,to keep the purity of test data.\n",
    "# train_family = index_to_use[0:(72557/10)*6]\n",
    "# validation_family = index_to_use[(72557/10)*6:(72557/10)*8]\n",
    "# test_family = index_to_use[(72557/10)*8:]\n",
    "# torch.save(train_family,'train_family.pkl')\n",
    "# torch.save(validation_family,'validation_family.pkl')\n",
    "# torch.save(test_family,'test_family.pkl')\n",
    "\n",
    "#load all the data from previous training\n",
    "train_family = torch.load('train_family.pkl')\n",
    "validation_family = torch.load('validation_family.pkl')\n",
    "test_family = torch.load('test_family.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#set all the hyperparameters(to be completed)\n",
    "EPOCH = 20\n",
    "LR = 0.01             \n",
    "batch = 40 \n",
    "\n",
    "net = Net_1()\n",
    "net.cuda()\n",
    "loss_func = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(net.parameters(), lr=LR)\n",
    "# net.load_state_dict(torch.load('params.pkl'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net.load_state_dict(torch.load('params_col_ba50_1.pkl'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# variation_loss = torch.load('variation_loss.pkl')\n",
    "# variation_loss_val = torch.load('variation_loss_val.pkl')\n",
    "\n",
    "#training section\n",
    "#load some datas & net parameters from previous training\n",
    "running_loss = 0.0\n",
    "running_loss_2 = 0.0\n",
    "variation_loss = []\n",
    "variation_loss_val = []\n",
    "test_history_accuracy = []\n",
    "test_history_accuracy_col = []\n",
    "test_history_accuracy_sty = []\n",
    "# variation_loss = torch.load('variation_loss_1.pkl')\n",
    "# variation_loss_val = torch.load('variation_loss_val_1.pkl')\n",
    "# test_history_accuracy = torch.load('test_history_accuracy.pkl')\n",
    "# test_history_accuracy_col = torch.load('test_history_accuracy_col.pkl')\n",
    "# test_history_accuracy_sty = torch.load('test_history_accuracy_sty.pkl')\n",
    "# net.load_state_dict(torch.load('params.pkl'))\n",
    "# print('paremeters loaded')\n",
    "\n",
    "for epoch in range(EPOCH):\n",
    "    net.eval()\n",
    "    signal = False\n",
    "    perbatch_index,num_batch = get_the_step_indexassignment(train_family,batch)\n",
    "    #get the epoch accuracy,due to the limitation of gpu memory,we could only use part of the test data to do the computation     \n",
    "    index_accur = validation_family\n",
    "    np.random.shuffle(index_accur)\n",
    "    n = len(index_accur)/15\n",
    "    x = Variable(get_x_for_thisbatch(index_accur[0:n],False)).cuda()\n",
    "    y_col = get_y_for_thisbatch(index_accur[0:n],ylabel_for_color).numpy()\n",
    "#     y_sty = get_y_for_thisbatch(index_accur[0:n],ylabel_for_style).numpy()\n",
    "    output_for_accuracy_col = net(x)\n",
    "    prediction_col = onehot2label(output_for_accuracy_col.cpu().data.numpy().T)\n",
    "#     prediction_sty = onehot2label(output_for_accuracy_sty.cpu().data.numpy().T)\n",
    "    accuracy_of_this_epoch_1 = accuracy(prediction_col,y_col)\n",
    "#     accuracy_of_this_epoch_2 = accuracy(prediction_sty,y_sty)\n",
    "    test_history_accuracy_col.append(accuracy_of_this_epoch_1)\n",
    "#     test_history_accuracy_sty.append(accuracy_of_this_epoch_2)\n",
    "#     accuracy_of_this_epoch = accuracy_of_this_epoch_2 + accuracy_of_this_epoch_1\n",
    "    print('accuracy of epoch for color',epoch+1,'=',accuracy_of_this_epoch_1)\n",
    "#     print('accuracy of epoch for style',epoch+1,'=',accuracy_of_this_epoch_2)\n",
    "#     torch.save(test_history_accuracy, 'test_history_accuracy.pkl')\n",
    "    torch.save(test_history_accuracy_col,'test_history_accuracy_col_ba50.pkl')\n",
    "    del x,y_col,output_for_accuracy_col\n",
    "     \n",
    "#     torch.save(test_history_accuracy_sty,'test_history_accuracy_sty.pkl')\n",
    "    #early stopping to avoid overfitting\n",
    "    if epoch > 0:\n",
    "        if accuracy_of_this_epoch_1 < (test_history_accuracy_col[-1]+test_history_accuracy_col[-2])/2:\n",
    "            print('early stopped')\n",
    "            break;\n",
    "    test_history_accuracy_col.append(accuracy_of_this_epoch_1)\n",
    "\n",
    "    #train the model\n",
    "    for step in range(num_batch*2):\n",
    "        net.train()\n",
    "        y_col = get_y_for_thisbatch(perbatch_index[step/2],ylabel_for_color)\n",
    "#         y_sty = get_y_for_thisbatch(perbatch_index[step/2],ylabel_for_style)\n",
    "        x = 0\n",
    "        x = get_x_for_thisbatch(perbatch_index[step/2],signal)\n",
    "        b_y_col = Variable(y_col).cuda()\n",
    "#         b_y_sty = Variable(y_sty).cuda()\n",
    "        b_x = Variable(x).cuda()\n",
    "        output_col = net(b_x)             \n",
    "        loss = loss_func(output_col, b_y_col)\n",
    "        del b_x,b_y_col,output_col\n",
    "        \n",
    "        optimizer.zero_grad()           \n",
    "        loss.backward()                 \n",
    "        optimizer.step()\n",
    "        \n",
    "        #get the variation of lost of validation set\n",
    "        #to save source,just compute partial batch size of it\n",
    "        arr = validation_family             \n",
    "        np.random.shuffle(arr)\n",
    "        arr = arr[0:batch/2]\n",
    "        net.eval()\n",
    "        yval_temp_col = get_y_for_thisbatch(arr,ylabel_for_color)\n",
    "#         yval_temp_sty = get_y_for_thisbatch(arr,ylabel_for_style)\n",
    "        xval_temp = get_x_for_thisbatch(arr,False)\n",
    "        b_y_col = Variable(yval_temp_col).cuda()\n",
    "#         b_y_sty = Variable(yval_temp_sty).cuda()\n",
    "        b_x_2 = Variable(xval_temp).cuda()\n",
    "        output_col = net(b_x_2)             \n",
    "        del b_x_2\n",
    "        loss_2 = loss_func(output_col, b_y_col)\n",
    "        del b_y_col,output_col\n",
    "        \n",
    "        running_loss += loss\n",
    "        running_loss_2 += loss_2\n",
    "        if step % 50 == 0:\n",
    "            #to avoid recording some bad points.\n",
    "            if running_loss.cpu().data[0] >= 5:\n",
    "                variation_loss.append(running_loss.cpu().data.numpy())\n",
    "            if running_loss_2.cpu().data[0] >= 5:\n",
    "                variation_loss_val.append(running_loss_2.cpu().data.numpy())\n",
    "                print(running_loss.cpu().data.numpy())                                      #loss per sample\n",
    "            running_loss = 0.0\n",
    "            running_loss_2 = 0.0\n",
    "            torch.save(net.state_dict(), 'params_col_ba50_1.pkl')  \n",
    "            torch.save(variation_loss, 'variation_loss_col_ba50_1.pkl')\n",
    "            torch.save(variation_loss_val, 'variation_loss_val_col_ba50_1.pkl')\n",
    "        #get some test prediction for comparsion \n",
    "        if step % 1000 == 0:\n",
    "            pre_select = test_family\n",
    "            np.random.shuffle(pre_select)\n",
    "            xtest = get_x_for_thisbatch(pre_select[:10],False)\n",
    "            x = Variable(xtest).cuda()\n",
    "            y_col = get_y_for_thisbatch(pre_select[:10],ylabel_for_color).numpy()\n",
    "#             y_sty = get_y_for_thisbatch(pre_select[:10],ylabel_for_style).numpy()\n",
    "            output_for_accuracy_col = net(x)\n",
    "            del x,xtest\n",
    "            prediction_col = onehot2label(output_for_accuracy_col.cpu().data.numpy().T)\n",
    "#             prediction_sty = onehot2label(output_for_accuracy_sty.cpu().data.numpy().T)\n",
    "\n",
    "            print('the original label for color is',y_col)\n",
    "            print('and the prediction for color is',prediction_col)\n",
    "#             print('----------------------------------------------------')\n",
    "#             print('the original label for style is',y_sty)\n",
    "#             print('and the prediction for style is',prediction_sty)\n",
    "    if signal == False:\n",
    "        signal = True\n",
    "    else:\n",
    "        signal = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_col = torch.load('variation_loss_col_seperate_epoch8.pkl')\n",
    "loss_val = torch.load('variation_loss_val_col_seperate_epoch80.pkl')\n",
    "xx = range(len(loss_col))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ll = torch.load('test_history_accuracy_col_ba50.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
